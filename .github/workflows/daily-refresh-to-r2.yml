name: Daily refresh and upload DBs to R2

on:
  schedule:
    - cron: "0 0 * * *"   # 00:00 UTC daily = 7:00 PM EST (8 PM EDT in summer)
  workflow_dispatch: {}

concurrency:
  group: daily-refresh
  cancel-in-progress: true

jobs:
  refresh:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
      R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
      R2_BUCKET: ${{ secrets.R2_BUCKET }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install awscli
        run: |
          python -m pip install --upgrade pip
          pip install awscli

      # Gate: allow only 00:00–00:59 UTC (buffer for delayed start)
      # Also run only once per UTC day (best-effort via refresh_status.json)
      - name: Gate to 7–8 PM EST (buffered)
        shell: bash
        run: |
          set -euo pipefail
          echo "UTC now: $(date -u)"

          NOW_UTC_HH=$(date -u +%H)

          if [[ "$NOW_UTC_HH" != "00" ]]; then
            echo "Outside 00:00–01:00 UTC window — skipping."
            exit 0
          fi

          mkdir -p /tmp/r2check
          set +e
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/refresh_status.json" \
            "/tmp/r2check/refresh_status.json" --no-progress
          RC=$?
          set -e

          if [[ $RC -eq 0 ]]; then
            LAST_UTC=$(python - <<'PY'
          import json
          with open("/tmp/r2check/refresh_status.json","r") as f:
              d=json.load(f)
          print(d.get("ran_at_utc",""))
          PY
            )

            if [[ -n "$LAST_UTC" ]]; then
              TODAY_UTC=$(date -u +%Y-%m-%d)
              LASTDAY_UTC=$(date -u -d "$LAST_UTC" +%Y-%m-%d 2>/dev/null || true)

              if [[ -n "$LASTDAY_UTC" && "$LASTDAY_UTC" == "$TODAY_UTC" ]]; then
                echo "Already ran today (UTC date: $TODAY_UTC) — skipping."
                exit 0
              fi
            fi
          else
            echo "No prior refresh_status.json found (first run or missing) — continuing."
          fi

          echo "Within allowed window and not yet run today — continuing."

      - name: Install deps
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install awscli

      - name: Prepare folders
        run: |
          mkdir -p backend/data

      - name: Download DB snapshots from R2
        shell: bash
        run: |
          set -e

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-in.db.gz" \
            backend/data/stockapp-in.db.gz --no-progress || \
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/stockapp-in.db.gz" \
            backend/data/stockapp-in.db.gz --no-progress

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-us.db.gz" \
            backend/data/stockapp-us.db.gz --no-progress || \
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/stockapp-us.db.gz" \
            backend/data/stockapp-us.db.gz --no-progress

          ls -lh backend/data

      - name: Decompress DBs
        run: |
          python - <<'PY'
          import gzip, shutil, os
          pairs = [
            ("backend/data/stockapp-in.db.gz", "backend/data/stockapp-in.db"),
            ("backend/data/stockapp-us.db.gz", "backend/data/stockapp-us.db"),
          ]
          for src, dst in pairs:
            if not os.path.exists(src):
              raise SystemExit(f"Missing {src}")
            with gzip.open(src, "rb") as fin, open(dst, "wb") as fout:
              shutil.copyfileobj(fin, fout)
            print("Wrote", dst, "size=", os.path.getsize(dst))
          PY

      - name: Debug help
        working-directory: backend
        run: |
          python -m app.run_universe_and_refresh --help

      - name: Refresh DB data
        working-directory: backend
        run: |
          python -m app.run_universe_and_refresh --market ALL daily

      - name: Create refresh_status.json
        run: |
          python - <<'PY'
          import sqlite3, json, os, datetime

          def stats(path):
            con = sqlite3.connect(path)
            cur = con.cursor()
            mx = cur.execute("SELECT MAX(date) FROM daily_bars").fetchone()[0]
            rows = cur.execute("SELECT COUNT(*) FROM daily_bars").fetchone()[0]
            latest_rows = cur.execute(
              "SELECT COUNT(*) FROM daily_bars WHERE date=? AND close IS NOT NULL", (mx,)
            ).fetchone()[0]
            con.close()
            return {
              "db": os.path.basename(path),
              "rows": rows,
              "max_date": mx,
              "rows_on_max_date_with_close": latest_rows,
            }

          out = {
            "ran_at_utc": datetime.datetime.utcnow().isoformat() + "Z",
            "india": stats("backend/data/stockapp-in.db"),
            "us": stats("backend/data/stockapp-us.db"),
          }

          with open("backend/refresh_status.json", "w") as f:
            json.dump(out, f, indent=2)

          print(json.dumps(out, indent=2))
          PY

      - name: Compress DBs
        run: |
          python - <<'PY'
          import gzip, shutil, os
          pairs = [
            ("backend/data/stockapp-in.db", "backend/data/stockapp-in.db.gz"),
            ("backend/data/stockapp-us.db", "backend/data/stockapp-us.db.gz"),
          ]
          for src, dst in pairs:
            with open(src, "rb") as fin, gzip.open(dst, "wb", compresslevel=6) as fout:
              shutil.copyfileobj(fin, fout)
            print("Wrote", dst, "size=", os.path.getsize(dst))
          PY

      - name: Upload updated snapshots + keep only Day-1 history
        shell: bash
        run: |
          set -euo pipefail

          TS=$(date -u +"%Y-%m-%dT%H-%M-%SZ")

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 rm "s3://$R2_BUCKET/snapshots/history/" --recursive || true

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-in.db.gz" \
            "s3://$R2_BUCKET/snapshots/history/$TS-stockapp-in.db.gz" --no-progress || true

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-us.db.gz" \
            "s3://$R2_BUCKET/snapshots/history/$TS-stockapp-us.db.gz" --no-progress || true

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            backend/data/stockapp-in.db.gz \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-in.db.gz" --no-progress

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            backend/data/stockapp-us.db.gz \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-us.db.gz" --no-progress

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            backend/refresh_status.json \
            "s3://$R2_BUCKET/snapshots/latest/refresh_status.json" --no-progress

          echo "=== latest/ ==="
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 ls "s3://$R2_BUCKET/snapshots/latest/"

          echo "=== history/ (should contain ONLY 2 objects) ==="
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 ls "s3://$R2_BUCKET/snapshots/history/"
