# Schedule

name: Daily refresh and upload DBs to R2

on:
  # GitHub cron is UTC-only. We run hourly and gate to 7:00 PM America/New_York
  schedule:
    - cron: "0 3 * * *"
  workflow_dispatch: {}

concurrency:
  group: daily-refresh
  cancel-in-progress: true

jobs:
  refresh:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
      R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
      R2_BUCKET: ${{ secrets.R2_BUCKET }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # Gate execution to exactly 7:00 PM US/Eastern (handles DST automatically)
      - name: Run only at 7 P
        shell: bash
        run: |
          set -euo pipefail
          echo "NY time now: $(TZ="America/New_York" date)"

          NOW_HH=$(TZ="America/New_York" date +%H)
          NOW_MM=$(TZ="America/New_York" date +%M)
          NOW_MM=$((10#$NOW_MM))

          in_7pm_window=false
          in_10pm_window=false

          # 7:00–7:30 PM ET
          if [[ "$NOW_HH" == "19" && "$NOW_MM" -le "30" ]]; then
            in_7pm_window=true
          fi

          if [[ "$in_7pm_window" != "true" ]]; then
            echo "Outside 7–7:30 PM ET window — skipping."
            exit 0
          fi

          echo "Within allowed window — continuing."

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        working-directory: backend
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install awscli

      - name: Prepare folders
        run: |
          mkdir -p backend/data

      # 1) Download baseline DB snapshots from R2 (prefer latest/, fallback to root)
      - name: Download DB snapshots from R2
        shell: bash
        run: |
          set -e

          # Try latest/ first
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-in.db.gz" \
            backend/data/stockapp-in.db.gz --no-progress || \
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/stockapp-in.db.gz" \
            backend/data/stockapp-in.db.gz --no-progress

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-us.db.gz" \
            backend/data/stockapp-us.db.gz --no-progress || \
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/stockapp-us.db.gz" \
            backend/data/stockapp-us.db.gz --no-progress

          ls -lh backend/data

      # 2) Decompress into .db
      - name: Decompress DBs
        run: |
          python - <<'PY'
          import gzip, shutil, os
          pairs = [
            ("backend/data/stockapp-in.db.gz", "backend/data/stockapp-in.db"),
            ("backend/data/stockapp-us.db.gz", "backend/data/stockapp-us.db"),
          ]
          for src, dst in pairs:
            if not os.path.exists(src):
              raise SystemExit(f"Missing {src}")
            with gzip.open(src, "rb") as fin, open(dst, "wb") as fout:
              shutil.copyfileobj(fin, fout)
            print("Wrote", dst, "size=", os.path.getsize(dst))
          PY

      # 3.0) Debug Step (keep if you want; safe to remove later)
      - name: Debug help
        working-directory: backend
        run: |
          python -m app.run_universe_and_refresh --help

      # 3) Refresh DB data (arg order matters: --market before subcommand)
      - name: Refresh DB data
        working-directory: backend
        run: |
          python -m app.run_universe_and_refresh --market ALL daily

      # 4) Create refresh_status.json
      - name: Create refresh_status.json
        run: |
          python - <<'PY'
          import sqlite3, json, os, datetime

          def stats(path):
            con = sqlite3.connect(path)
            cur = con.cursor()
            mx = cur.execute("SELECT MAX(date) FROM daily_bars").fetchone()[0]
            rows = cur.execute("SELECT COUNT(*) FROM daily_bars").fetchone()[0]
            latest_rows = cur.execute(
              "SELECT COUNT(*) FROM daily_bars WHERE date=? AND close IS NOT NULL", (mx,)
            ).fetchone()[0]
            con.close()
            return {
              "db": os.path.basename(path),
              "rows": rows,
              "max_date": mx,
              "rows_on_max_date_with_close": latest_rows,
            }

          out = {
            "ran_at_utc": datetime.datetime.utcnow().isoformat() + "Z",
            "india": stats("backend/data/stockapp-in.db"),
            "us": stats("backend/data/stockapp-us.db"),
          }

          with open("backend/refresh_status.json", "w") as f:
            json.dump(out, f, indent=2)

          print(json.dumps(out, indent=2))
          PY

      # 5) Compress updated DBs back to .gz
      - name: Compress DBs
        run: |
          python - <<'PY'
          import gzip, shutil, os
          pairs = [
            ("backend/data/stockapp-in.db", "backend/data/stockapp-in.db.gz"),
            ("backend/data/stockapp-us.db", "backend/data/stockapp-us.db.gz"),
          ]
          for src, dst in pairs:
            with open(src, "rb") as fin, gzip.open(dst, "wb", compresslevel=6) as fout:
              shutil.copyfileobj(fin, fout)
            print("Wrote", dst, "size=", os.path.getsize(dst))
          PY

      # 6) Keep ONLY Day-1 history + update latest
      - name: Upload updated snapshots + keep only Day-1 history
        shell: bash
        run: |
          set -euo pipefail

          # Timestamp format for history filenames:
          # 2026-01-26T04-06-00Z-stockapp-in.db.gz
          TS=$(date -u +"%Y-%m-%dT%H-%M-%SZ")

          # A) Delete any existing history objects so history ends up with ONLY 2 objects total
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 rm "s3://$R2_BUCKET/snapshots/history/" --recursive || true

          # B) Copy yesterday's latest into history (Day-1). If latest doesn't exist (first run), ignore errors.
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-in.db.gz" \
            "s3://$R2_BUCKET/snapshots/history/$TS-stockapp-in.db.gz" --no-progress || true

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-us.db.gz" \
            "s3://$R2_BUCKET/snapshots/history/$TS-stockapp-us.db.gz" --no-progress || true

          # C) Upload today's refreshed files to latest (overwrite)
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            backend/data/stockapp-in.db.gz \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-in.db.gz" --no-progress

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            backend/data/stockapp-us.db.gz \
            "s3://$R2_BUCKET/snapshots/latest/stockapp-us.db.gz" --no-progress

          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 cp \
            backend/refresh_status.json \
            "s3://$R2_BUCKET/snapshots/latest/refresh_status.json" --no-progress

          echo "=== latest/ ==="
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 ls "s3://$R2_BUCKET/snapshots/latest/"

          echo "=== history/ (should contain ONLY 2 objects) ==="
          python -m awscli --endpoint-url "$R2_ENDPOINT" s3 ls "s3://$R2_BUCKET/snapshots/history/"
